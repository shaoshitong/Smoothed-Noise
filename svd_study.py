import numpy as np
import torch
import einops

# [905.675537109375, 6.78812313079834, 6.72459077835083, 6.685277938842773, 6.6806230545043945, 6.653902530670166, 6.627554416656494, 6.612252235412598, 6.600739479064941, 6.569337844848633, 6.564811706542969, 6.556906700134277, 6.544941425323486, 6.54000997543335, 6.521800518035889, 6.510158538818359, 6.501594543457031, 6.494302749633789, 6.487264156341553, 6.459813594818115, 6.448958873748779, 6.446892261505127, 6.427694320678711, 6.418364524841309, 6.412949085235596, 6.400667190551758, 6.389334678649902, 6.37930965423584, 6.367938995361328, 6.345561981201172, 6.336271286010742, 6.321035861968994, 6.316886901855469, 6.306277751922607, 6.3001813888549805, 6.289311408996582, 6.278530597686768, 6.268226623535156, 6.2332048416137695, 6.222734451293945, 6.215742111206055, 6.2086639404296875, 6.186883449554443, 6.177649021148682, 6.158646106719971, 6.144316673278809, 6.135162353515625, 6.1180620193481445, 6.0848774909973145, 6.031927585601807, 0.9453006982803345]

# [896.7694702148438, 6.7880940437316895, 6.724582195281982, 6.685234546661377, 6.680575847625732, 6.65388822555542, 6.62753438949585, 6.612205982208252, 6.600729942321777, 6.569322109222412, 6.564813613891602, 6.556918621063232, 6.544930458068848, 6.540009498596191, 6.521759033203125, 6.510144233703613, 6.501575946807861, 6.494297504425049, 6.487242221832275, 6.459806442260742, 6.448970794677734, 6.446901321411133, 6.427700519561768, 6.418334484100342, 6.412939071655273, 6.4006428718566895, 6.389352321624756, 6.379287242889404, 6.367924690246582, 6.345530033111572, 6.336262226104736, 6.321053504943848, 6.31687593460083, 6.306258678436279, 6.300175189971924, 6.289290428161621, 6.278520584106445, 6.268191337585449, 6.233217716217041, 6.222733497619629, 6.215732097625732, 6.208669662475586, 6.186891555786133, 6.177614212036133, 6.158638954162598, 6.144326686859131, 6.135154724121094, 6.118058681488037, 6.0848708152771, 6.031919002532959]
if __name__ == "__main__":
    ensemble_latents = torch.from_numpy(np.load("./ensemble.npz.npy")).squeeze(1)
    original_latents = torch.from_numpy(np.load("./latents.npz.npy"))
    
    total_latents = ensemble_latents.cuda() # torch.cat([original_latents,ensemble_latents],0).cuda()
    b, c, f, h, w = total_latents.shape
    total_latents = einops.rearrange(total_latents, "b c f h w -> (b f) c h w")
    total_latents = torch.nn.functional.interpolate(total_latents,size=[h//4,w//4])
    total_latents = einops.rearrange(total_latents, "(b f) c h w -> b c f h w",b=b,f=f).contiguous()
    total_latents = total_latents.reshape(total_latents.shape[0],-1)
    print(total_latents.shape)
    s, v, d = torch.linalg.svd(total_latents.float())
    print(v.tolist())
    